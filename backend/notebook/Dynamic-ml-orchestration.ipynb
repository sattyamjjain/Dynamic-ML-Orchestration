{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc02e8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Configure S3 details\n",
    "s3_bucket = 'pyverseai'  # Replace with your S3 bucket name\n",
    "file_key = 'dataset/Reviews.csv'  # Replace with your file key in S3\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Function to load dataset from S3\n",
    "def load_dataset_from_s3(bucket_name, file_key):\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    data = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    return data\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset_from_s3(s3_bucket, file_key)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91e8ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Score Sentiment\n",
      "0  I have bought several of the Vitality canned d...      5  positive\n",
      "1  Product arrived labeled as Jumbo Salted Peanut...      1  negative\n",
      "2  This is a confection that has been around a fe...      4  positive\n",
      "3  If you are looking for the secret ingredient i...      2  negative\n",
      "4  Great taffy at a great price.  There was a wid...      5  positive\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "dataset = dataset.dropna(subset=['Text', 'Score'])\n",
    "\n",
    "# Keep only the relevant columns\n",
    "dataset = dataset[['Text', 'Score']]\n",
    "\n",
    "# Create a new column 'Sentiment' based on the rating\n",
    "# For simplicity, we'll assume Score > 3 is positive, 3 is neutral, and < 3 is negative\n",
    "def sentiment_label(score):\n",
    "    if score > 3:\n",
    "        return 'positive'\n",
    "    elif score < 3:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "dataset['Sentiment'] = dataset['Score'].apply(sentiment_label)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca029240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  I have bought several of the Vitality canned d...   \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "2  This is a confection that has been around a fe...   \n",
      "3  If you are looking for the secret ingredient i...   \n",
      "4  Great taffy at a great price.  There was a wid...   \n",
      "\n",
      "                                        Cleaned_Text Sentiment  \n",
      "0  bought several vitality canned dog food produc...  positive  \n",
      "1  product arrived labeled jumbo salted peanuts p...  negative  \n",
      "2  confection around centuries light pillowy citr...  positive  \n",
      "3  looking secret ingredient robitussin believe f...  negative  \n",
      "4  great taffy great price wide assortment yummy ...  positive  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already present\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to the 'Text' column\n",
    "dataset['Cleaned_Text'] = dataset['Text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(dataset[['Text', 'Cleaned_Text', 'Sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf822419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (568454, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "X = vectorizer.fit_transform(dataset['Cleaned_Text'])\n",
    "\n",
    "# Display the shape of the transformed data\n",
    "print(\"Shape of TF-IDF matrix:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3df11754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (454763, 5000)\n",
      "Testing set shape: (113691, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the target variable (Sentiment) and features (TF-IDF matrix)\n",
    "y = dataset['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd16f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8677995619706045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.68      0.71     16181\n",
      "     neutral       0.51      0.18      0.27      8485\n",
      "    positive       0.90      0.97      0.93     89025\n",
      "\n",
      "    accuracy                           0.87    113691\n",
      "   macro avg       0.71      0.61      0.64    113691\n",
      "weighted avg       0.85      0.87      0.85    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)  # max_iter can be adjusted based on the dataset size\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7944009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved in 'model_dir'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "import tarfile\n",
    "\n",
    "# Create a directory to save the model and vectorizer\n",
    "os.makedirs('model_dir', exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'model_dir/sentiment_model.pkl')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'model_dir/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"Model and vectorizer saved in 'model_dir'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5053d047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to: s3://pyverseai/dynamic-ml-orchestration/models/sentiment_model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Create a tar.gz file for the model and vectorizer\n",
    "with tarfile.open('sentiment_model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('model_dir/sentiment_model.pkl', arcname='sentiment_model.pkl')\n",
    "    archive.add('model_dir/tfidf_vectorizer.pkl', arcname='tfidf_vectorizer.pkl')\n",
    "\n",
    "# Define the S3 bucket and file paths\n",
    "model_tar_path = 'sentiment_model.tar.gz'\n",
    "s3_model_path = 'dynamic-ml-orchestration/models/sentiment_model.tar.gz'\n",
    "\n",
    "# Upload the tar.gz file to S3\n",
    "s3.upload_file(model_tar_path, s3_bucket, s3_model_path)\n",
    "\n",
    "print(f\"Model uploaded to: s3://{s3_bucket}/{s3_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93c3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
